{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T17:25:53.840050Z",
     "start_time": "2024-03-12T17:25:50.405923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\r\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\r\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (8.1.7)\r\n",
      "Collecting joblib (from nltk)\r\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\r\n",
      "Collecting regex>=2021.8.3 (from nltk)\r\n",
      "  Downloading regex-2023.12.25-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m40.9/40.9 kB\u001B[0m \u001B[31m1.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting tqdm (from nltk)\r\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m57.6/57.6 kB\u001B[0m \u001B[31m4.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.5/1.5 MB\u001B[0m \u001B[31m5.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading regex-2023.12.25-cp312-cp312-macosx_11_0_arm64.whl (292 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m292.2/292.2 kB\u001B[0m \u001B[31m6.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m302.2/302.2 kB\u001B[0m \u001B[31m7.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.3/78.3 kB\u001B[0m \u001B[31m4.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: tqdm, regex, joblib, nltk\r\n",
      "Successfully installed joblib-1.3.2 nltk-3.8.1 regex-2023.12.25 tqdm-4.66.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --Tokeniizing --\n",
    "### `from nltk.tokenize import wrod_tokenize,sent_tokenize`\n",
    "## Word tokenizers : seprated by words\n",
    "## sentence tokenizers : seprated by sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''NLP represents  the automatic handling of natural human language like speech or text, and although the concept itself is fascinating, the real value behind this technology comes from the use cases.\n",
    "NLP can help you with lots of tasks and the fields of application just seem to increase on a daily basis.'''\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- STOP WORDS--\n",
    "### `from nltk.corpus import stopwords`\n",
    "stop words: these are those words which are not important in analysing the data.\n",
    "    example: and,hi,he,she,etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "text = word_tokenize(text)\n",
    "filtered_text =' '.join([word for word in text if not word in set(stopwords.words(\"english\"))])\n",
    "filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --Stemming--\n",
    "### `from nltk.stem import PorterStemmer`\n",
    "is is basically removal of plural and adjectives\n",
    "#### ex:  \n",
    "* loved--> love \n",
    "* learning -->learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "ps = PorterStemmer()\n",
    "example_words = ['earn',\"earning\",\"earned\",\"earns\",\"History\"]\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "ps = PorterStemmer()\n",
    "example_words = [\"earn earning\",\"earned\",\"earns\",'formality','changes']\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "ps = PorterStemmer()\n",
    "texts = ' '\n",
    "for w in text:\n",
    "    texts =texts+' '+ ps.stem(w)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --Lemmatization--\n",
    "\n",
    "Convert the word into an meaningful sentense . \n",
    "\n",
    "ex : chatbots , q&ans\n",
    "```\n",
    "--stemming--\n",
    "history -> histori\n",
    "\n",
    "--lemmatization--\n",
    "history -> history\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [lemmatizer.lemmatize(word) for word in text if word not in set(stopwords.words('english'))]\n",
    "' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() ## Create object for lemmatizer\n",
    "example_words = ['history','formality','changes']\n",
    "for w in example_words:\n",
    "    print(lemmatizer.lemmatize(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --WordNet--\n",
    "\n",
    "WordNet is the lexical database i.e. dictionary for the English language, specifically designed for natural language processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "syns = wordnet.synsets(\"program\")\n",
    "#synset\n",
    "#print(syns[0].name())\n",
    "#print(syns[0].lemmas()[0].name())\n",
    "\n",
    "##definition\n",
    "\n",
    "#print(syns[0].definition())\n",
    "#print(syns[0].examples())\n",
    "\n",
    "synonyms = []\n",
    "antonyms =[]\n",
    "\n",
    "for syn in wordnet.synsets(\"happy\"):\n",
    "    for i in syn.lemmas():\n",
    "        synonyms.append(i.name())\n",
    "        if i.antonyms():\n",
    "            antonyms.append(i.antonyms()[0].name())\n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --Bag of Words--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is used to convert the text data into vectors . \n",
    "\n",
    "```\n",
    "sent1 = he is a good boy\n",
    "sent2 = she is a good girl\n",
    "sent3 = boy and girl are good \n",
    "        |\n",
    "        |\n",
    "  After removal of stopwords , lematization or stemming\n",
    "sent1 = good boy\n",
    "sent2 = good girl\n",
    "sent3 = boy girl good  \n",
    "        | ### Now we will calculate the frequency for each word by\n",
    "        |     calculating the occurrence of each word\n",
    "word  frequency\n",
    "good     3\n",
    "boy      2\n",
    "girl     2\n",
    "         | ## Then according to their occurrence we assign o or 1 \n",
    "         |    according to their occurrence in the sentence 1 for                      |     present and 0 fot not present\n",
    "         \n",
    "         f1  f2   f3\n",
    "        girl good boy   o/p\n",
    "sent1    0    1    1    \n",
    "sent2    1    0    1\n",
    "sent3    1    1    1\n",
    "```   \n",
    "\n",
    "### Disadvantage\n",
    "In bag of words every word given either a 1 or 0 . 1 means present in the sentense and 0 means not . using this approach we can not find out that which word is more important and which one is less important . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sent = pd.DataFrame(['he is a good boy', 'she is a good girl', 'boy and girl are good'],columns=['text'])\n",
    "print(sent)\n",
    "print(len(sent))\n",
    "corpus = []\n",
    "for i in range(0,3):\n",
    "    words = sent['text'][i]\n",
    "    words  = word_tokenize(words)\n",
    "    texts = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    text = ' '.join(texts)\n",
    "    corpus.append(text)\n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --TFIDF Vectorization--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is used to convert the text data into vectors . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Bag of Words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer(max_features=10)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
