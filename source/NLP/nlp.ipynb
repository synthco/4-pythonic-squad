{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "        \n",
    "isw_vect_upd = pd.read_csv('isw_vect_upd.csv')\n",
    "isw_vect_upd\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "480fdf73e8318173",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pure = isw_vect_upd[[\"text_for_vect2\", \"date\"]]\n",
    "pure"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2eafa1c3c5c4c45",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg\n",
    "# !python -m spacy download en_core_web_sm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "429b522e31b7e012",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import spacy\n",
    "# import en_core_web_sm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
    "    return lemmatized_text\n",
    "\n",
    "pure['text_for_vect2'] = pure['text_for_vect2'].apply(lemmatize_text)\n",
    "pure\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "409f36698655f6d0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def remove_smth(text):\n",
    "    stops = [\"yet\", \"zaporizhzhia\", \"we\", \"y\",]\n",
    "    words = text.split()\n",
    "    filtered_words = [w for w in words if w not in stops]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "pure['text_for_vect2'] = pure[\"text_for_vect2\"].apply(remove_smth)\n",
    "pure "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32fde6c44c107390",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(max_features=700)\n",
    "tfidf_matrix = tfidf_vect.fit_transform(pure['text_for_vect2'])\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vect.get_feature_names_out())\n",
    "vect_df = pd.concat([pure[\"date\"], tfidf_df], axis=1)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43f481c59f32bed",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9d5cb94b87fab37d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# vect_df.to_csv('isw_tfidf.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e0f9091cef0c84",
   "execution_count": 0
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
